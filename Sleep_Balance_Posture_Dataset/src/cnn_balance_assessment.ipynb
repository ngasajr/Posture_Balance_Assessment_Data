{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0dface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import os, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, regularizers\n",
    "\n",
    "from scipy.signal import resample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"TensorFlow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8fa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Parameters\n",
    "\n",
    "# REQUIRED: set this to the root directory of dataset.\n",
    "DATASET_ROOT = \"PATH/TO/DATASET_ROOT\"\n",
    "\n",
    "TEST_FOLDERS = [\"test1\", \"test2\", \"test3\"]\n",
    "FILE_GLOBS   = [\"*.csv\", \"*.xlsx\", \"*.xls\"]\n",
    "\n",
    "# Sequence handling\n",
    "TARGET_LEN   = 1024   # resampled/padded length per sample\n",
    "MIN_LEN_OK   = 64     # skip samples with fewer timesteps than this\n",
    "USE_RESAMPLE = True   # True: resample time axis to TARGET_LEN; False: pad/truncate\n",
    "FILL_NA      = True\n",
    "FILL_VALUE   = 0.0\n",
    "SCALE_PER_FEATURE = True  # standardize features (fit on TRAIN folds only)\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE   = 32\n",
    "EPOCHS       = 50      # Increase or decrease based on your results analysis\n",
    "VAL_FRAC     = 0.2\n",
    "PATIENCE     = 12\n",
    "SEED         = 42\n",
    "\n",
    "# Labels\n",
    "LABEL_MAP   = {\"test1\":0, \"test2\":1, \"test3\":2}\n",
    "LABEL_NAMES = [\"Open\", \"Closed\", \"Dynamic\"]\n",
    "N_CLASSES   = len(LABEL_MAP)\n",
    "\n",
    "# Output directory\n",
    "SAVE_DIR = \"./cnn_balance_results\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26376ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover Files in Folders\n",
    "def list_files(dataset_root, test_folders, file_globs):\n",
    "    if not os.path.isdir(dataset_root):\n",
    "        raise FileNotFoundError(\n",
    "            f\"DATASET_ROOT does not exist: {dataset_root}\\n\"\n",
    "            \"Please set DATASET_ROOT to the path containing test1/test2/test3.\"\n",
    "        )\n",
    "    samples = []\n",
    "    for folder in test_folders:\n",
    "        dir_path = os.path.join(dataset_root, folder)\n",
    "        if not os.path.isdir(dir_path):\n",
    "            raise FileNotFoundError(f\"Missing folder: {dir_path}\")\n",
    "        files = []\n",
    "        for pat in file_globs:\n",
    "            files += glob.glob(os.path.join(dir_path, pat))\n",
    "        for fp in sorted(files):\n",
    "            samples.append((fp, LABEL_MAP[folder]))\n",
    "    print(f\"Found {len(samples)} files total.\")\n",
    "    return samples\n",
    "\n",
    "all_files = list_files(DATASET_ROOT, TEST_FOLDERS, FILE_GLOBS)\n",
    "assert len(all_files) > 0, \"No files found. Check DATASET_ROOT and folder structure.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e61572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read one Excel/CSV to 2D array [time, F]\n",
    "def read_excel_like(path, fill_na=True, fill_value=0.0):\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    else:\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "    # Drop fully empty columns\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # Remove 'time' column if present\n",
    "    lower_cols = [c.lower() for c in df.columns]\n",
    "    if \"time\" in lower_cols:\n",
    "        tcol = df.columns[lower_cols.index(\"time\")]\n",
    "        df = df.drop(columns=[tcol])\n",
    "\n",
    "    # Convert to numeric\n",
    "    for c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    if fill_na:\n",
    "        df = df.fillna(fill_value)\n",
    "\n",
    "    arr = df.values.astype(np.float32)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[:, None]  # [T] -> [T,1]\n",
    "    return arr  # [T, F]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix sample length (resample/pad/trunc)\n",
    "def fix_length(x, target_len=1024, min_len_ok=64, use_resample=True):\n",
    "    T, F = x.shape\n",
    "    if T < min_len_ok:\n",
    "        return None\n",
    "    if use_resample:\n",
    "        x_fixed = resample(x, target_len, axis=0)\n",
    "    else:\n",
    "        if T >= target_len:\n",
    "            x_fixed = x[:target_len]\n",
    "        else:\n",
    "            pad = np.zeros((target_len - T, F), dtype=x.dtype)\n",
    "            x_fixed = np.vstack([x, pad])\n",
    "    return x_fixed.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset arrays X (N,T,F), y (N,)\n",
    "X_list, y_list, name_list = [], [], []\n",
    "\n",
    "for fp, y in all_files:\n",
    "    arr = read_excel_like(fp, fill_na=FILL_NA, fill_value=FILL_VALUE)  # [T, F]\n",
    "    fixed = fix_length(arr, TARGET_LEN, MIN_LEN_OK, USE_RESAMPLE)\n",
    "    if fixed is None:\n",
    "        print(\"Skipping short file:\", os.path.basename(fp))\n",
    "        continue\n",
    "    X_list.append(fixed)\n",
    "    y_list.append(y)\n",
    "    name_list.append(fp)\n",
    "\n",
    "X = np.array(X_list)                  # [N, T, F]\n",
    "y = np.array(y_list)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape, \"samples:\", len(name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd13c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (compact ResNet-1D + regulariz.)\n",
    "def res_block(x, filters, kernel=7, wd=1e-4):\n",
    "    shortcut = x\n",
    "    x = layers.Conv1D(filters, kernel, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.Conv1D(filters, 3, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding=\"same\",\n",
    "                                 kernel_regularizer=regularizers.l2(wd))(shortcut)\n",
    "        shortcut = layers.BatchNormalization()(shortcut)\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    return x\n",
    "\n",
    "def build_resnet1d(input_shape, n_classes, wd=1e-4,\n",
    "                   label_smooth=0.05, noise_std=0.01, dropout=0.35):\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.GaussianNoise(noise_std)(inp)\n",
    "\n",
    "    x = layers.Conv1D(64, 7, padding=\"same\",\n",
    "                      kernel_regularizer=regularizers.l2(wd))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "\n",
    "    x = res_block(x, 128, kernel=5, wd=wd)\n",
    "    x = res_block(x, 256, kernel=3, wd=wd)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "\n",
    "    out = layers.Dense(n_classes, activation=\"softmax\",\n",
    "                       kernel_regularizer=regularizers.l2(wd))(x)\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    opt  = tf.keras.optimizers.Adam(3e-4)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smooth)\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "input_shape = (X.shape[1], X.shape[2])\n",
    "print(\"Model input shape:\", input_shape, \"classes:\", N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: splits, scaling, metrics, CM\n",
    "def stratified_val_split(X_tr, y_tr, val_frac=0.2, seed=42):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_frac, random_state=seed)\n",
    "    idx_tr2, idx_val = next(sss.split(X_tr, y_tr))\n",
    "    return X_tr[idx_tr2], y_tr[idx_tr2], X_tr[idx_val], y_tr[idx_val]\n",
    "\n",
    "def fit_scaler_on_train(X_tr):\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X_tr.reshape(-1, X_tr.shape[-1]))\n",
    "    return sc\n",
    "\n",
    "def apply_scaler(sc, Z):\n",
    "    z = Z.reshape(-1, Z.shape[-1])\n",
    "    z = sc.transform(z)\n",
    "    return z.reshape(Z.shape)\n",
    "\n",
    "def to_onehot(y, n_classes):\n",
    "    oh = np.zeros((len(y), n_classes), dtype=np.float32)\n",
    "    oh[np.arange(len(y)), y] = 1.0\n",
    "    return oh\n",
    "\n",
    "def metrics_tuple(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def save_normalized_cm(y_true, y_pred, labels, title, out_prefix, show_colorbar=False):\n",
    "    # Uses matplotlib only and avoids specifying colors/styles explicitly.\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels).astype(np.float32)\n",
    "    cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = np.nan_to_num(cm_norm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.0, 4.5))\n",
    "    im = ax.imshow(cm_norm, interpolation='nearest')  # no explicit colormap\n",
    "\n",
    "    if show_colorbar:\n",
    "        ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    ax.set(\n",
    "        xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "        xticklabels=LABEL_NAMES[:len(labels)], yticklabels=LABEL_NAMES[:len(labels)],\n",
    "        ylabel='True Label', xlabel='Predicted Label', title=title\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\")\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ax.text(j, i, format(cm_norm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm_norm[i, j] > thresh else \"black\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    pdf_path = os.path.join(SAVE_DIR, f\"{out_prefix}.pdf\")\n",
    "    png_path = os.path.join(SAVE_DIR, f\"{out_prefix}.png\")\n",
    "    plt.savefig(pdf_path, bbox_inches='tight')\n",
    "    plt.savefig(png_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved CM:\", pdf_path, \"and\", png_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fa638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-class 5-fold CV + Confusion Matrix\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_stats = []\n",
    "all_true, all_pred = [], []\n",
    "\n",
    "for fold, (tr_idx, te_idx) in enumerate(skf.split(X, y), start=1):\n",
    "    X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "    y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "    # Scale per-feature on training data\n",
    "    scaler = fit_scaler_on_train(X_tr)\n",
    "    X_tr = apply_scaler(scaler, X_tr)\n",
    "    X_te = apply_scaler(scaler, X_te)\n",
    "\n",
    "    # Stratified validation split\n",
    "    X_tr2, y_tr2, X_val, y_val = stratified_val_split(X_tr, y_tr, val_frac=VAL_FRAC, seed=fold)\n",
    "\n",
    "    # One-hot labels for label-smoothing\n",
    "    y_tr2_oh = to_onehot(y_tr2, N_CLASSES)\n",
    "    y_val_oh = to_onehot(y_val, N_CLASSES)\n",
    "\n",
    "    model = build_resnet1d(input_shape, N_CLASSES, wd=1e-4, label_smooth=0.05,\n",
    "                           noise_std=0.01, dropout=0.35)\n",
    "\n",
    "    ckpt_path = os.path.join(SAVE_DIR, f\"best_fold{fold}.keras\")\n",
    "    cbs = [\n",
    "        callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\",\n",
    "                                  save_best_only=True, verbose=0),\n",
    "        callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n",
    "                                    patience=5, verbose=0),\n",
    "        callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=PATIENCE,\n",
    "                                restore_best_weights=True, verbose=0)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        X_tr2, y_tr2_oh,\n",
    "        validation_data=(X_val, y_val_oh),\n",
    "        epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "        verbose=0, callbacks=cbs, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Evaluate on test split\n",
    "    y_prob = model.predict(X_te, verbose=0)\n",
    "    y_hat  = np.argmax(y_prob, axis=1)\n",
    "\n",
    "    acc, prec, rec, f1 = metrics_tuple(y_te, y_hat)\n",
    "    print(f\"FOLD {fold}: Acc={acc:.3f} Prec={prec:.3f} Rec={rec:.3f} F1={f1:.3f}\")\n",
    "    print(confusion_matrix(y_te, y_hat))\n",
    "    print(classification_report(y_te, y_hat, target_names=LABEL_NAMES, zero_division=0))\n",
    "\n",
    "    fold_stats.append([acc, prec, rec, f1])\n",
    "    all_true.append(y_te)\n",
    "    all_pred.append(y_hat)\n",
    "\n",
    "fold_stats = np.array(fold_stats)\n",
    "print(\"\\n=== 3-Class CNN (5-fold) — Mean ± SD ===\")\n",
    "print(f\"Acc  : {fold_stats[:,0].mean():.3f} ± {fold_stats[:,0].std():.3f}\")\n",
    "print(f\"Prec : {fold_stats[:,1].mean():.3f} ± {fold_stats[:,1].std():.3f}\")\n",
    "print(f\"Rec  : {fold_stats[:,2].mean():.3f} ± {fold_stats[:,2].std():.3f}\")\n",
    "print(f\"F1   : {fold_stats[:,3].mean():.3f} ± {fold_stats[:,3].std():.3f}\")\n",
    "\n",
    "all_true = np.concatenate(all_true)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "\n",
    "# Save normalized CM (3-class) as PDF + PNG\n",
    "save_normalized_cm(\n",
    "    all_true, all_pred, labels=[0,1,2],\n",
    "    title=\"CNN - Balance Assessment (3 Classes)\",\n",
    "    out_prefix=\"cnn_balance_cm_3class\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise helpers and 5-fold CV\n",
    "def subset_pair(X, y, a, b):\n",
    "    mask = (y == a) | (y == b)\n",
    "    Xp, yp = X[mask], y[mask]\n",
    "    # map labels to 0/1 (a->0, b->1)\n",
    "    yp = (yp == b).astype(np.int32)\n",
    "    return Xp, yp\n",
    "\n",
    "def run_pairwise_cv(X, y, class_a, class_b, title, fname_prefix):\n",
    "    print(f\"\\n--- Pairwise: {title} ---\")\n",
    "    Xp, yp = subset_pair(X, y, class_a, class_b)\n",
    "    if len(np.unique(yp)) < 2:\n",
    "        print(\"Not enough classes in subset.\")\n",
    "        return\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    fold_stats = []\n",
    "    all_true, all_pred = [], []\n",
    "\n",
    "    for fold, (tr, te) in enumerate(skf.split(Xp, yp), start=1):\n",
    "        X_tr, X_te = Xp[tr], Xp[te]\n",
    "        y_tr, y_te = yp[tr], yp[te]\n",
    "\n",
    "        scaler = fit_scaler_on_train(X_tr)\n",
    "        X_tr = apply_scaler(scaler, X_tr)\n",
    "        X_te = apply_scaler(scaler, X_te)\n",
    "\n",
    "        X_tr2, y_tr2, X_val, y_val = stratified_val_split(X_tr, y_tr, val_frac=VAL_FRAC, seed=fold)\n",
    "        y_tr2_oh = to_onehot(y_tr2, 2)\n",
    "        y_val_oh = to_onehot(y_val, 2)\n",
    "\n",
    "        model = build_resnet1d((X_tr.shape[1], X_tr.shape[2]), 2,\n",
    "                               wd=1e-4, label_smooth=0.05, noise_std=0.01, dropout=0.35)\n",
    "\n",
    "        ckpt_path = os.path.join(SAVE_DIR, f\"{fname_prefix}_best_fold{fold}.keras\")\n",
    "        cbs = [\n",
    "            callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_accuracy\",\n",
    "                                      save_best_only=True, verbose=0),\n",
    "            callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5,\n",
    "                                        patience=5, verbose=0),\n",
    "            callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=PATIENCE,\n",
    "                                    restore_best_weights=True, verbose=0)\n",
    "        ]\n",
    "\n",
    "        model.fit(X_tr2, y_tr2_oh,\n",
    "                  validation_data=(X_val, y_val_oh),\n",
    "                  epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  verbose=0, callbacks=cbs, shuffle=True)\n",
    "\n",
    "        y_hat = np.argmax(model.predict(X_te, verbose=0), axis=1)\n",
    "        acc, prec, rec, f1 = metrics_tuple(y_te, y_hat)\n",
    "        print(f\"Fold {fold}: Acc={acc:.3f} Prec={prec:.3f} Rec={rec:.3f} F1={f1:.3f}\")\n",
    "        print(confusion_matrix(y_te, y_hat))\n",
    "\n",
    "        fold_stats.append([acc, prec, rec, f1])\n",
    "        all_true.append(y_te); all_pred.append(y_hat)\n",
    "\n",
    "    fold_stats = np.array(fold_stats)\n",
    "    print(f\"\\n{title} — Mean ± SD\")\n",
    "    print(f\"Acc  : {fold_stats[:,0].mean():.3f} ± {fold_stats[:,0].std():.3f}\")\n",
    "    print(f\"Prec : {fold_stats[:,1].mean():.3f} ± {fold_stats[:,1].std():.3f}\")\n",
    "    print(f\"Rec  : {fold_stats[:,2].mean():.3f} ± {fold_stats[:,2].std():.3f}\")\n",
    "    print(f\"F1   : {fold_stats[:,3].mean():.3f} ± {fold_stats[:,3].std():.3f}\")\n",
    "\n",
    "    all_true = np.concatenate(all_true)\n",
    "    all_pred = np.concatenate(all_pred)\n",
    "\n",
    "    # Save normalized CM for the pair (mapped to 0/1)\n",
    "    save_normalized_cm(\n",
    "        all_true, all_pred, labels=[0,1],\n",
    "        title=f\"CNN - {title}\",\n",
    "        out_prefix=f\"{fname_prefix}_cm\"\n",
    "    )\n",
    "\n",
    "# Run pairwise analyses\n",
    "run_pairwise_cv(X, y, LABEL_MAP[\"test1\"], LABEL_MAP[\"test2\"],\n",
    "                title=\"Test1 (Open) vs Test2 (Closed)\",\n",
    "                fname_prefix=\"pair_1v2\")\n",
    "\n",
    "run_pairwise_cv(X, y, LABEL_MAP[\"test1\"], LABEL_MAP[\"test3\"],\n",
    "                title=\"Test1 (Open) vs Test3 (Dynamic)\",\n",
    "                fname_prefix=\"pair_1v3\")\n",
    "\n",
    "run_pairwise_cv(X, y, LABEL_MAP[\"test2\"], LABEL_MAP[\"test3\"],\n",
    "                title=\"Test2 (Closed) vs Test3 (Dynamic)\",\n",
    "                fname_prefix=\"pair_2v3\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
